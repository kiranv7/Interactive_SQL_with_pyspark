{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install PySpark**"
      ],
      "metadata": {
        "id": "nCcyvYSWUesG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZlEtSmBEa69",
        "outputId": "a2d016d5-8092-42a4-961b-5bc6f61f174c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=17b5dd2abe70b253e9e3de663e0397476738de532f7b1bc1946fda1caf9e7eef\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "# install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Create a connection with the SQL Server from here at PySpark**\n",
        "\n",
        "note1: This step assumes you have SQL Server and it is operational\n",
        "\n",
        "note2: Create a database. For example, it can be called 'user_database'\n",
        "\n",
        "note3: Create tables in that database. For example, a table called 'user_table\n",
        "\n",
        "note4: Populate the table with data of your choice. For example, adding data about employees.\n",
        "\n",
        "\n",
        "**Step 4: Make sure to replace the names with the correct ones**\n",
        "\n",
        "user_name = 'my_user' should be exactly the name you registered SQL Server.\n",
        "\n",
        "Password = 'user1234' should be exactly your password you set in the SQL Server.\n",
        "\n",
        "server_name = 'my_server' should be exactly your SQL server name indicated in your connection\n",
        "\n",
        "PORT = '1433' should be exactly the port number to access.\n",
        "\n",
        "database_name = 'my_database' should be exactly the name of your database how you named it SQL server."
      ],
      "metadata": {
        "id": "kT5kg3jKVJjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connection setup  -----> add username, password, server_name, and database_name\n",
        "username = \"dbo\"\n",
        "password = \"user1234\"\n",
        "server_name = \"myPass\"\n",
        "port = \"1433\"\n",
        "database_name = \"my_database\"\n",
        "\n",
        "jdbc_url = \"jdbc:sqlserver://\" + server_name + \":\" + port +\";database=\" + database_name\n",
        "\n",
        "connectionProperties = {\n",
        "            \"user\": username,\n",
        "            \"password\": password,\n",
        "            }\n"
      ],
      "metadata": {
        "id": "AybOgFsUUXWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Now the connection has been established. Proceed with executing the commands**"
      ],
      "metadata": {
        "id": "6aOfYfr-YeQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start the pyspark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"SQL Database Connection\").getOrCreate()"
      ],
      "metadata": {
        "id": "9ylfx5UKYdbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the data types to be used in the database\n",
        "\n",
        "from pyspark.sql.types import StringType, StructField, StructType, IntegerType, FloatType, DoubleType, BooleanType, DateType, TimestampType\n",
        "from pyspark.sql.functions import col, when\n",
        "import datetime"
      ],
      "metadata": {
        "id": "BT0UpA5iZC_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, execute the code and get to create the tables and modify them\n",
        "\n",
        "# Prompt the user to create a table in the SQL database\n",
        "create_table = input(\"Do you want to create a table in the SQL database or edit/view an existing table? (C/E)\")\n",
        "\n",
        "if create_table.lower() != \"c\" and create_table.lower() != \"e\":\n",
        "    print(\"No table created.\")\n",
        "        \n",
        "else:\n",
        "        if create_table.lower() == \"c\":\n",
        "            # Prompt the user for the name of the table and the column names\n",
        "            table_name = input(\"Enter the name of the table:\")\n",
        "            # take column names and data types from user\n",
        "            column_data = input(\n",
        "            \"Enter column names and data types (separated by commas, e.g. col1:int,col2:string):\").split(\",\")\n",
        "            \n",
        "            fields = []\n",
        "            for col_data in column_data:\n",
        "                col_name, col_type = col_data.split(\":\")\n",
        "                if col_type == \"int\":\n",
        "                    fields.append(StructField(col_name, IntegerType(), True))\n",
        "                elif col_type == \"float\":\n",
        "                    fields.append(StructField(col_name, FloatType(), True))\n",
        "                elif col_type == \"double\":\n",
        "                    fields.append(StructField(col_name, DoubleType(), True))\n",
        "                elif col_type == \"boolean\":\n",
        "                    fields.append(StructField(col_name, BooleanType(), True))\n",
        "                elif col_type == \"date\":\n",
        "                    fields.append(StructField(col_name, DateType(), True))\n",
        "                elif col_type == \"timestamp\":\n",
        "                    fields.append(StructField(col_name, TimestampType(), True))\n",
        "                else:\n",
        "                    fields.append(StructField(col_name, StringType(), True))\n",
        "\n",
        "            # create DataFrame schema based on column names and data types\n",
        "            schema = StructType(fields)\n",
        "\n",
        "            # create empty DataFrame with schema\n",
        "            df = spark.createDataFrame([], schema)\n",
        "\n",
        "            # Write the DataFrame to the SQL database\n",
        "            df.write.format(\"jdbc\").options(\n",
        "                url=jdbc_url,\n",
        "                dbtable=table_name,\n",
        "                user=username,\n",
        "                password=password).mode(\"overwrite\").save()\n",
        "\n",
        "            print(f\"Table {table_name} created in the SQL database.\")\n",
        "        else: \n",
        "            print(\"Choose one of the following tables by writing its name:\")\n",
        "            # Retrieve table names\n",
        "            table_names = spark.read.jdbc(\n",
        "                url=jdbc_url, table=\"INFORMATION_SCHEMA.TABLES\", properties=connectionProperties).\\\n",
        "                            filter(\"TABLE_TYPE = 'BASE TABLE'\").select(\"TABLE_NAME\").collect()\n",
        "\n",
        "            # Print table names\n",
        "            for tableName in table_names:\n",
        "                print(tableName.TABLE_NAME)\n",
        "                \n",
        "            table_name = input(\"Enter table name: \")\n",
        "            \n",
        "\n",
        "        # Prompt the user to add or delete entries in the table\n",
        "        while True:\n",
        "            df = spark.read.jdbc(url=jdbc_url,\n",
        "                                     table=table_name, properties=connectionProperties).persist()\n",
        "\n",
        "            schema = df.schema\n",
        "            \n",
        "            # create an empty list to store the column data\n",
        "            column_data = []\n",
        "\n",
        "            # loop through each column in the schema\n",
        "            for field in schema.fields:\n",
        "                # get the name and data type of the column\n",
        "                col_name = field.name\n",
        "                col_type = field.dataType\n",
        "\n",
        "                # map the data type to a string representation\n",
        "                if isinstance(col_type, IntegerType):\n",
        "                    data_type_str = \"int\"\n",
        "                elif isinstance(col_type, FloatType):\n",
        "                    data_type_str = \"float\"\n",
        "                elif isinstance(col_type, DoubleType):\n",
        "                    data_type_str = \"double\"\n",
        "                elif isinstance(col_type, BooleanType):\n",
        "                    data_type_str = \"boolean\"\n",
        "                elif isinstance(col_type, DateType):\n",
        "                    data_type_str = \"date\"\n",
        "                elif isinstance(col_type, TimestampType):\n",
        "                    data_type_str = \"timestamp\"\n",
        "                else:\n",
        "                    data_type_str = \"string\"\n",
        "\n",
        "                # append the column name and data type string to the list\n",
        "                column_data.append(f\"{col_name}:{data_type_str}\")\n",
        "            \n",
        "            # ask user if they want to add or delete an entry\n",
        "            modify_entry = input(\n",
        "                \"Do you want to add, delete, update an entry (A/D/U), print the table (P),\" +\n",
        "                \"or switch/create a table (S)?\")\n",
        "\n",
        "            if modify_entry.lower() == \"a\":\n",
        "\n",
        "                # create empty dictionary to store entry data\n",
        "                entry_data = {}\n",
        "\n",
        "                # take input for each column in the table\n",
        "                for col_data in column_data:\n",
        "                    col_name, col_type = col_data.split(\":\")\n",
        "                    entry_value = input(f\"Enter value for {col_name}: \")\n",
        "                    if col_type == \"int\":\n",
        "                        entry_data[col_name] = int(entry_value)\n",
        "                    elif col_type == \"float\" or col_type == \"double\":\n",
        "                        entry_data[col_name] = float(entry_value)\n",
        "                    elif col_type == \"boolean\":\n",
        "                        entry_data[col_name] = bool(entry_value)\n",
        "                    elif col_type == \"date\":\n",
        "                        entry_data[col_name] = datetime.strptime(entry_value, \"%Y-%m-%d\").date()\n",
        "                    elif col_type == \"timestamp\":\n",
        "                        entry_data[col_name] = datetime.strptime(entry_value, \"%Y-%m-%d %H:%M:%S\")\n",
        "                    else:\n",
        "                        entry_data[col_name] = entry_value\n",
        "\n",
        "                # create DataFrame with new entry\n",
        "                new_entry = spark.createDataFrame([entry_data], schema)\n",
        "\n",
        "                # append it to original DataFrame\n",
        "                df = df.union(new_entry)\n",
        "\n",
        "                print(f\"New row added to table {table_name}.\")\n",
        "                \n",
        "            elif modify_entry.lower() == \"d\":\n",
        "                # take column name and value to delete from user\n",
        "                print(\"Column names:\", df.columns)\n",
        "                delete_col = input(\"Enter column name to delete entry from: \")\n",
        "                delete_value = input(\"Enter value to delete: \")\n",
        "\n",
        "                # search for column type\n",
        "                for col_data in column_data:\n",
        "                    col_name, col_type = col_data.split(\":\")\n",
        "\n",
        "                    if delete_col != col_name:\n",
        "                      continue\n",
        "\n",
        "                    if col_type == \"int\":\n",
        "                        delete_value = int(delete_value)\n",
        "                    elif col_type == \"float\" or col_type == \"double\":\n",
        "                        delete_value = float(delete_value)\n",
        "                    elif col_type == \"boolean\":\n",
        "                        delete_value = bool(delete_value)\n",
        "                    elif col_type == \"date\":\n",
        "                        delete_value = datetime.strptime(delete_value, \"%Y-%m-%d\").date()\n",
        "                    elif col_type == \"timestamp\":\n",
        "                        delete_value = datetime.strptime(delete_value, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "                if delete_col in df.columns:\n",
        "                    # delete entry from DataFrame\n",
        "                    df = df.filter(f\"{delete_col} != '{delete_value}'\")\n",
        "\n",
        "                else:\n",
        "                    print(\"Column is not found.\")\n",
        "\n",
        "            elif modify_entry.lower() == \"u\":\n",
        "\n",
        "                # take column name and value to update from user\n",
        "                print(\"Column names:\", df.columns)\n",
        "                update_col = input(\"Enter column name to update entry: \")\n",
        "                update_value = input(\"Enter value to update: \")\n",
        "                update_col_new_value = input(\"Enter new value: \")\n",
        "\n",
        "                # search for column type\n",
        "                for col_data in column_data:\n",
        "                    col_name, col_type = col_data.split(\":\")\n",
        "\n",
        "                    if update_col != col_name:\n",
        "                      continue\n",
        "\n",
        "                    if col_type == \"int\":\n",
        "                        update_col_new_value = int(update_col_new_value)\n",
        "                        update_value = int(update_value)\n",
        "                    elif col_type == \"float\":\n",
        "                        update_col_new_value = float(update_col_new_value)\n",
        "                        update_value = float(update_value)\n",
        "                    elif col_type == \"boolean\":\n",
        "                        update_col_new_value = bool(update_col_new_value)\n",
        "                        update_value = bool(update_value)\n",
        "                    elif col_type == \"date\":\n",
        "                        update_col_new_value = datetime.strptime(update_col_new_value, \"%Y-%m-%d\").date()\n",
        "                        update_value = datetime.strptime(update_value, \"%Y-%m-%d\").date()\n",
        "                    elif col_type == \"timestamp\":\n",
        "                        update_col_new_value = datetime.strptime(update_col_new_value, \"%Y-%m-%d %H:%M:%S\")\n",
        "                        update_value = datetime.strptime(update_value, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "                # update the entry in the DataFrame\n",
        "                df = df.withColumn(update_col, when(col(update_col) == update_value, \n",
        "                                                    update_col_new_value).otherwise(col(update_col)))\n",
        "\n",
        "            elif modify_entry.lower() == \"p\":\n",
        "                # show the contents of the DataFrame\n",
        "                df.show()\n",
        "\n",
        "            elif modify_entry.lower() == \"s\":\n",
        "                \n",
        "                print(\"Choose one of the following tables by writing its name:\")\n",
        "                # Retrieve table names\n",
        "                table_names = spark.read.jdbc(\n",
        "                url=jdbc_url, table=\"INFORMATION_SCHEMA.TABLES\", properties=connectionProperties).\\\n",
        "                            filter(\"TABLE_TYPE = 'BASE TABLE'\").select(\"TABLE_NAME\").collect()\n",
        "\n",
        "                # Store table names in a list\n",
        "                tables = [tableName.TABLE_NAME for tableName in table_names]\n",
        "                \n",
        "                # Print table names\n",
        "                print(tables)\n",
        "                \n",
        "                switch_table = input(\"Enter table name to switch to or enter 'C' to create a new table: \")\n",
        "                \n",
        "                # check if the user wants to create new table\n",
        "                if switch_table.lower() == \"c\":\n",
        "                    # take table name\n",
        "                    table_name = input(\"Enter table name: \")\n",
        "\n",
        "                    # take column names and data types from user\n",
        "                    column_data = input(\"Enter column names and data types (separated by commas, e.g.\" + \n",
        "                                        \"col1:int,col2:string):\").split(\",\")\n",
        "\n",
        "                    fields = []\n",
        "                    for col_data in column_data:\n",
        "                        col_name, col_type = col_data.split(\":\")\n",
        "                        if col_type == \"int\":\n",
        "                            fields.append(StructField(col_name, IntegerType(), True))\n",
        "                        elif col_type == \"float\" or col_type == \"double\":\n",
        "                            fields.append(StructField(col_name, FloatType(), True))\n",
        "                        elif col_type == \"boolean\":\n",
        "                            fields.append(StructField(col_name, BooleanType(), True))\n",
        "                        elif col_type == \"date\":\n",
        "                            fields.append(StructField(col_name, DateType(), True))\n",
        "                        elif col_type == \"timestamp\":\n",
        "                            fields.append(StructField(col_name, TimestampType(), True))\n",
        "                        else:\n",
        "                            fields.append(StructField(col_name, StringType(), True))\n",
        "\n",
        "                    # create DataFrame schema based on column names and data types\n",
        "                    schema = StructType(fields)\n",
        "                    \n",
        "                    # create empty DataFrame with schema\n",
        "                    df = spark.createDataFrame([], schema)\n",
        "\n",
        "                # check if table exists in dictionary\n",
        "                elif switch_table in tables:\n",
        "                    # switch to selected table\n",
        "                    table_name = switch_table\n",
        "\n",
        "                else:\n",
        "                    print(\"Table does not exist.\")\n",
        "                    continue\n",
        "\n",
        "            else:\n",
        "              print(\"Invalid input.\")\n",
        "\n",
        "            if modify_entry.lower() != \"p\" and modify_entry.lower() != \"s\":\n",
        "                \n",
        "                # Write the updated table to the SQL database\n",
        "                df.write.format(\"jdbc\").options(url=jdbc_url, \n",
        "                                     dbtable=table_name, user=username,\n",
        "                                password=password, truncate = True).mode(\"overwrite\").save()\n",
        "\n",
        "\n",
        "            # ask user if they want to continue modifying the table\n",
        "            continue_modifying = input(\"Do you want to continue modifying the table? (Y/N) \")\n",
        "\n",
        "            if continue_modifying.lower() != \"y\":\n",
        "              break"
      ],
      "metadata": {
        "id": "bcrmZ7lIZRo-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c0d42c0-c411-4c49-d51f-d8248ff251a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to create a table in the SQL database or edit/view an existing table? (C/E)c\n",
            "Enter the name of the table:bd\n",
            "Enter column names and data types (separated by commas, e.g. col1:int,col2:string):name:string,id:int\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9e0538d55673>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Write the DataFrame to the SQL database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             df.write.format(\"jdbc\").options(\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mdbtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.save.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    }
  ]
}